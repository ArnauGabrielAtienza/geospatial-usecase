{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudbutton Geospatial: Water Consumption Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:27.706148Z",
     "start_time": "2021-04-13T14:38:27.027515Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from cloudbutton_geospatial.io_utils.plot import plot_results\n",
    "from cloudbutton_geospatial.utils.notebook import date_picker\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from shapely.geometry import Point, MultiPoint, box\n",
    "from pprint import pprint\n",
    "import functools\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lithops\n",
    "import requests\n",
    "import rasterio\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import utm\n",
    "import tempfile\n",
    "import concurrent.futures\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from lithops.storage import Storage\n",
    "from lithops.storage.utils import StorageNoSuchKeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area outside the processed tile that we want to consider for taking SIAM stations into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:28.795793Z",
     "start_time": "2021-04-13T14:38:28.788173Z"
    }
   },
   "outputs": [],
   "source": [
    "AREA_OF_INFLUENCE = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lithops Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:29.658886Z",
     "start_time": "2021-04-13T14:38:29.654251Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_BUCKET = 'cloudbutton-geospatial-wc'\n",
    "COMPUTE_BACKEND = 'localhost'\n",
    "STORAGE_BACKEND = 'localhost'\n",
    "STORAGE_PREFIX = 'localhost://'\n",
    "RUNTIME = 'aitorarjona/cloudbutton-geospatial-wc:01'\n",
    "RUNTIME_MEMORY = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM_ASC_PREFIX = 'DTMs/asc/'\n",
    "DTM_GEOTIFF_PREFIX = 'DTMs/geotiff/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split tile into square chunks (number of tiles = SPLITS^2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:30.525082Z",
     "start_time": "2021-04-13T14:38:30.519883Z"
    }
   },
   "outputs": [],
   "source": [
    "SPLITS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation coefficient between elevation and temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:31.362634Z",
     "start_time": "2021-04-13T14:38:31.359578Z"
    }
   },
   "outputs": [],
   "source": [
    "r = -0.0056"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elevation to interpolate temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:32.187402Z",
     "start_time": "2021-04-13T14:38:32.184798Z"
    }
   },
   "outputs": [],
   "source": [
    "zdet = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day of year to calculate solar irradiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date_picker(default=datetime.date(2022, 5, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:33.304420Z",
     "start_time": "2021-04-13T14:38:33.299098Z"
    }
   },
   "outputs": [],
   "source": [
    "DAY_OF_YEAR = date.value.timetuple().tm_yday\n",
    "DAY_OF_YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = lithops.storage.Storage(backend=STORAGE_BACKEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fexec = lithops.FunctionExecutor(backend=COMPUTE_BACKEND, storage=STORAGE_BACKEND, runtime=RUNTIME, runtime_memory=RUNTIME_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get meteorological data for selected date (12:00h) and station locations from Catalonia Open Data portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XEMA_DATA_URL = 'https://analisi.transparenciacatalunya.cat/resource/nzvn-apee.json'\n",
    "XEMA_META_STATIONS = 'https://analisi.transparenciacatalunya.cat/resource/yqwd-vj5e.json'\n",
    "XEMA_META_VARS = 'https://analisi.transparenciacatalunya.cat/resource/4fb2-n3yi.json'\n",
    "\n",
    "@functools.cache\n",
    "def get_meteo_stations():\n",
    "    if os.path.exists('meta_cache/meteo_stations.csv'):\n",
    "        print('Getting meteo station from local cache')\n",
    "        with open('meta_cache/meteo_stations.csv', 'r') as f:\n",
    "            stations_meta = pd.read_csv(f)\n",
    "        return stations_meta\n",
    "    \n",
    "    print('Fetching meteo station from analisi.transparenciacatalunya.cat...')\n",
    "    res = requests.get(XEMA_META_STATIONS, params={'NOM_ESTAT': f'Operativa'})\n",
    "    stations_meta = pd.DataFrame(res.json())\n",
    "    return stations_meta\n",
    "\n",
    "@functools.cache\n",
    "def get_meteo_variables():\n",
    "    if os.path.exists('meta_cache/meteo_variables.csv'):\n",
    "        print('Getting meteo variables from local cache')\n",
    "        with open('meta_cache/meteo_variables.csv', 'r') as f:\n",
    "            vars_meta = pd.read_csv(f)\n",
    "        return vars_meta\n",
    "            \n",
    "    print('Fetching meteo station from analisi.transparenciacatalunya.cat...')\n",
    "    res = requests.get(XEMA_META_VARS)\n",
    "    vars_meta = pd.DataFrame(res.json())\n",
    "    return vars_meta\n",
    "    \n",
    "def get_meteo_data(date, stations_meta, vars_meta):\n",
    "    if os.path.exists(f'meta_cache/meteo-meta-{date.isoformat()}.csv'):\n",
    "        print('Getting meteo variables from local cache')\n",
    "        with open(f'meta_cache/meteo-meta-{date.isoformat()}.csv', 'r') as f:\n",
    "            vars_meta = pd.read_csv(f)\n",
    "        return vars_meta\n",
    "    res = requests.get(XEMA_DATA_URL, params={'data_lectura': f'{date.isoformat()}T12:00:00.000'})\n",
    "    xema_day = pd.DataFrame(res.json()).set_index('id')\n",
    "    \n",
    "    merged = xema_day.merge(stations_meta, on=\"codi_estacio\").merge(vars_meta, on=\"codi_variable\")\n",
    "    merged = merged[[\"codi_estacio\", \"nom_estacio\", \"latitud\", \"longitud\", \"acronim\", \"valor_lectura\"]]\n",
    "    \n",
    "    merged_transposed = (merged.groupby(['codi_estacio', 'nom_estacio', 'latitud', 'longitud'])\n",
    "                         .apply(lambda group: group[['acronim', 'valor_lectura']].set_index('acronim').T.reset_index(drop=True).rename_axis(None, axis=1))\n",
    "                         .reset_index().drop(['level_4'], axis=1).set_index('codi_estacio'))\n",
    "    return merged_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_stations = get_meteo_stations()\n",
    "meteo_vars = get_meteo_variables()\n",
    "meteo_data = get_meteo_data(date.value, meteo_stations, meteo_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_vars[['acronim', 'nom_variable', 'unitat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_coords = pd.DataFrame(meteo_data.apply(lambda row: utm.from_latlon(row['latitud'], row['longitud']), axis=1).to_list(),\n",
    "                          columns=['X', 'Y', 'zone_num', 'zone_letter'])\n",
    "x_y_coords = x_y_coords.drop(['zone_num', 'zone_letter'], axis=1)\n",
    "meteo_data = pd.concat([meteo_data, x_y_coords], axis=1)\n",
    "meteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store metadata to local cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('meta_cache/meteo_stations.csv'):\n",
    "    with open('meta_cache/meteo_stations.csv', 'w') as f:\n",
    "        meteo_stations.to_csv(f)\n",
    "\n",
    "if not os.path.exists('meta_cache/meteo_variables.csv'):\n",
    "    with open('meta_cache/meteo_variables.csv', 'w') as f:\n",
    "        meteo_vars.to_csv(f)\n",
    "        \n",
    "if not os.path.exists(f'meta_cache/meteo-meta-{date.value.isoformat()}.csv'):\n",
    "    with open(f'meta_cache/meteo-meta-{date.value.isoformat()}.csv', 'w') as f:\n",
    "        meteo_data.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload metadata to Cloud Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_key = f'meteo-meta-{date.value.isoformat()}.csv'\n",
    "try:\n",
    "    storage.head_object(bucket=DATA_BUCKET, key=meteo_key)\n",
    "except StorageNoSuchKeyError:\n",
    "    print('Uploading meteo data to Object Storage...')\n",
    "    body = meteo_data.to_csv().encode('utf-8')\n",
    "    storage.put_object(bucket=DATA_BUCKET, key=meteo_key, body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital Terrain Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download DTM files for free from http://centrodedescargas.cnig.es/CentroDescargas/buscadorCatalogo.do?codFamilia=MDT05# and put them in `inpit_DTMs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_asc_keys = storage.list_keys(bucket=DATA_BUCKET, prefix=DTM_ASC_PREFIX)\n",
    "dtm_asc_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload tiles from local input folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:48:12.000399Z",
     "start_time": "2021-04-07T09:48:11.986192Z"
    }
   },
   "outputs": [],
   "source": [
    "local_dtm_input = 'input_DTMs'\n",
    "local_dtms = [os.path.join(local_dtm_input, dtm) for dtm in os.listdir(local_dtm_input) if dtm.endswith('.asc')]\n",
    "\n",
    "def upload_file(file_path):\n",
    "    key = os.path.join(DTM_ASC_PREFIX, os.path.basename(file_path))\n",
    "    if key in dtm_asc_keys:\n",
    "        print(f'Tile {key} already in storage')\n",
    "        return key\n",
    "    with open(file_path, 'rb') as f:\n",
    "        print(f'Uploading {key}...')\n",
    "        storage.put_object(bucket=DATA_BUCKET, key=key, body=f)\n",
    "    return key\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=16) as pool:\n",
    "    result = list(pool.map(upload_file, local_dtms))\n",
    "    # list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asc_to_geotiff(obj, storage):\n",
    "    asc_file_name = os.path.basename(obj.key)\n",
    "    tile_id, _ = os.path.splitext(asc_file_name)\n",
    "    out_path = os.path.join(tempfile.gettempdir(), tile_id + '.tiff')\n",
    "    out_key = os.path.join(DTM_GEOTIFF_PREFIX, tile_id + '.tiff')\n",
    "    \n",
    "    try:\n",
    "        out_obj = storage.head_object(bucket=DATA_BUCKET, key=out_key)\n",
    "    except StorageNoSuchKeyError:\n",
    "        out_obj = None\n",
    "    \n",
    "    if out_obj:\n",
    "        print(f'GeoTIFF {tile_id} already exists, skipping...')\n",
    "        return out_key\n",
    "    \n",
    "    print(f'Converting {tile_id} to GeoTIFF...')\n",
    "    with rasterio.open(obj.data_stream, 'r') as src:\n",
    "        profile = src.profile\n",
    "        # Cloud optimized GeoTiff parameters\n",
    "        profile.update(driver='GTiff')\n",
    "        profile.update(blockxsize=256)\n",
    "        profile.update(blockysize=256)\n",
    "        profile.update(tiled=True)\n",
    "        profile.update(compress='deflate')\n",
    "        profile.update(interleave='band')\n",
    "        with rasterio.open(out_path, 'w', **profile) as dest:\n",
    "            dest.write(src.read())\n",
    "    \n",
    "    with open(out_path, 'rb') as f:\n",
    "        storage.put_object(bucket=DATA_BUCKET, key=out_key, body=f)\n",
    "    \n",
    "    return out_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fexec.map(asc_to_geotiff, os.path.join(STORAGE_PREFIX, DATA_BUCKET, DTM_ASC_PREFIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = fexec.wait(fs=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_geotiff_keys = storage.list_keys(bucket=DATA_BUCKET, prefix=DTM_GEOTIFF_PREFIX)\n",
    "dtm_geotiff_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:48:13.697506Z",
     "start_time": "2021-04-07T09:48:13.691877Z"
    }
   },
   "outputs": [],
   "source": [
    "tile_ids = [os.path.splitext(os.path.basename(dtm))[0] for dtm in dtm_geotiff_keys]\n",
    "tile_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert digital elevation map into a Cloud Optimized Geotiff. Upload then to IBM COS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serverless computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data tiles in subtiles for increased parallelism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_chunker(obj, n_splits, block_x, block_y, storage):\n",
    "    tile_key = os.path.basename(obj.key)\n",
    "    tile_id, _ = os.path.splitext(tile_key)\n",
    "    \n",
    "    with rasterio.open(obj.data_stream) as src:\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Compute working window\n",
    "        step_w = src.width / n_splits\n",
    "        step_h = src.height / n_splits\n",
    "        \n",
    "        offset_h = round(step_h * block_x)\n",
    "        offset_w = round(step_w * block_y)\n",
    "        \n",
    "        profile = src.profile\n",
    "        \n",
    "        width = math.ceil(step_w * (block_y + 1) - offset_w)\n",
    "        height = math.ceil(step_h * (block_x + 1) - offset_h)\n",
    "        \n",
    "        profile.update(width=width)\n",
    "        profile.update(height=height)\n",
    "        \n",
    "        window = Window(offset_w,offset_h, width, height)\n",
    "        \n",
    "        chunk_file = os.path.join(tempfile.gettempdir(), tile_id + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "        with rasterio.open(chunk_file, 'w', **profile) as dest:\n",
    "            dest.write(src.read(window=window))\n",
    "    \n",
    "    with open(chunk_file, 'rb') as f:\n",
    "        co = storage.put_cloudobject(body=f, bucket=DATA_BUCKET)\n",
    "    \n",
    "    return (tile_key, block_x, block_y, co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata = [(os.path.join(STORAGE_PREFIX, DATA_BUCKET, tile), SPLITS, i, j) for i in range(SPLITS) for j in range(SPLITS) for tile in dtm_geotiff_keys]\n",
    "iterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker_fs = fexec.map(data_chunker, iterdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = fexec.get_result(fs=chunker_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute solar irradiation given a day of year using GRASS libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:54.462039Z",
     "start_time": "2021-04-13T14:38:54.449706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_solar_irradiation(inputFile, outputFile, crs='32630'):\n",
    "    # Define grass working set\n",
    "    GRASS_GISDB = 'grassdata'\n",
    "    GRASS_LOCATION = 'GEOPROCESSING'\n",
    "    GRASS_MAPSET = 'PERMANENT'\n",
    "    GRASS_ELEVATIONS_FILENAME = 'ELEVATIONS'\n",
    "\n",
    "    os.environ['GRASSBIN'] = 'grass76'\n",
    "    from grass_session import Session\n",
    "    import grass.script as gscript\n",
    "    from grass.pygrass.modules.shortcuts import general\n",
    "    from grass.pygrass.modules.shortcuts import raster\n",
    "\n",
    "    os.environ.update(dict(GRASS_COMPRESS_NULLS='1'))\n",
    "    \n",
    "    # Clean previously processed data\n",
    "    if os.path.isdir(GRASS_GISDB):\n",
    "        shutil.rmtree(GRASS_GISDB)\n",
    "    \n",
    "    with Session(gisdb=GRASS_GISDB, location=GRASS_LOCATION, mapset=GRASS_MAPSET, create_opts='EPSG:32630') as ses:\n",
    "        # Set project projection to match elevation raster projection\n",
    "        general.proj(epsg=crs, flags='c') \n",
    "    \n",
    "        # Load raster file into working directory\n",
    "        raster.import_(input=inputFile, output=GRASS_ELEVATIONS_FILENAME, flags='o')    \n",
    "        \n",
    "        # Set project region to match raster region\n",
    "        general.region(raster=GRASS_ELEVATIONS_FILENAME, flags='s')    \n",
    "        # Calculate solar irradiation\n",
    "        gscript.run_command('r.slope.aspect', elevation=GRASS_ELEVATIONS_FILENAME,\n",
    "                            slope='slope', aspect='aspect')\n",
    "        gscript.run_command('r.sun', elevation=GRASS_ELEVATIONS_FILENAME,\n",
    "                            slope='slope', aspect='aspect', beam_rad='beam',\n",
    "                            step=1, day=DAY_OF_YEAR)\n",
    "        \n",
    "        # Get extraterrestrial irradiation from history metadata\n",
    "        regex = re.compile(r'\\d+\\.\\d+')\n",
    "        output = gscript.read_command(\"r.info\", flags=\"h\", map=[\"beam\"])\n",
    "        splits = str(output).split('\\n')\n",
    "        line = next(filter(lambda line: 'Extraterrestrial' in line, splits))\n",
    "        extraterrestrial_irradiance = float(regex.search(line)[0])\n",
    "        \n",
    "        # Export generated results into a GeoTiff file\n",
    "        if os.path.isfile(outputFile):\n",
    "            os.remove(outputFile)\n",
    "\n",
    "        raster.out_gdal(input='beam', output=outputFile)\n",
    "        \n",
    "        return extraterrestrial_irradiance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stations contained in the area of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:55.461635Z",
     "start_time": "2021-04-13T14:38:55.457230Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_stations(bounds, stations):\n",
    "    total_points = MultiPoint([Point(x,y) for x, y in stations[['X', 'Y']].to_numpy()])\n",
    "    intersection = bounds.buffer(AREA_OF_INFLUENCE).intersection(total_points)\n",
    "    \n",
    "    return stations[[ intersection.contains(point) for point in total_points]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Distance Weighting interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:58.209269Z",
     "start_time": "2021-04-13T14:38:58.202597Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_basic_interpolation(shape, stations, field_value, offset = (0,0)):\n",
    "    station_pixels = [[pixel[0], pixel[1]] for pixel in stations['pixel'].to_numpy()]\n",
    "    \n",
    "    # Get an array where each position represents pixel coordinates\n",
    "    tile_pixels = np.indices(shape).transpose(1,2,0).reshape(shape[0]*shape[1], 2) + offset\n",
    "    dist = distance_matrix(station_pixels, tile_pixels)\n",
    "    weights = np.where(dist == 0, np.finfo('float32').max, 1.0 / dist )\n",
    "    weights /=  weights.sum(axis=0)\n",
    "    \n",
    "    return np.dot(weights.T, stations[field_value].to_numpy()).reshape(shape).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate temperatures from a subset of the tile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:59.031150Z",
     "start_time": "2021-04-13T14:38:59.005158Z"
    }
   },
   "outputs": [],
   "source": [
    "def radiation_interpolation(tile_key, block_x, block_y, chunk_cloudobject, storage):\n",
    "    tile_id, _ = os.path.splitext(tile_key)\n",
    "    \n",
    "    # Write tile chunk to file\n",
    "    chunk_file = os.path.join(tempfile.gettempdir(), tile_id + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "    with open(chunk_file, 'wb') as f:\n",
    "        body = storage.get_cloudobject(chunk_cloudobject)\n",
    "        f.write(body)\n",
    "    \n",
    "    with rasterio.open(chunk_file, 'r') as chunk_src:\n",
    "        profile = chunk_src.profile\n",
    "        \n",
    "    extr_chunk_file = os.path.join(tempfile.gettempdir(), tile_id + '_extr_' + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "    rad_chunk_file = os.path.join(tempfile.gettempdir(), tile_id + '_rad_' + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "    \n",
    "    # Compute solar irradiation from inputFile, creates radiation raster at outputFile\n",
    "    extraterrestrial_irradiation = compute_solar_irradiation(inputFile=chunk_file, outputFile=rad_chunk_file)\n",
    "        \n",
    "    # Create and store a raster with extraterrestrial irradiation\n",
    "    with rasterio.open(extr_chunk_file, 'w', **profile) as dest:\n",
    "        data = np.full((profile['height'], profile['width']), extraterrestrial_irradiation, dtype='float32')\n",
    "        dest.write(data, 1)\n",
    "    \n",
    "    with open(extr_chunk_file, 'rb') as f:\n",
    "        extr_co = storage.put_cloudobject(body=f, bucket=DATA_BUCKET)\n",
    "    \n",
    "    with open(rad_chunk_file, 'rb') as f:\n",
    "        rad_co = storage.put_cloudobject(body=f, bucket=DATA_BUCKET)\n",
    "    \n",
    "    return [(tile_key, 'extr', block_x, block_y, extr_co), (tile_key, 'rad', block_x, block_y, rad_co)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:39:01.538202Z",
     "start_time": "2021-04-13T14:39:01.521080Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_interpolation(tile_key, block_x, block_y, chunk_cloudobject, data_field, storage):    \n",
    "    tile_id, _ = os.path.splitext(tile_key)\n",
    "    \n",
    "    # Get meteo data CSV\n",
    "    meteo_data_stream = storage.get_object(DATA_BUCKET, meteo_key, stream=True)\n",
    "    meteo_data = pd.read_csv(meteo_data_stream)\n",
    "    \n",
    "    # Write tile chunk to file\n",
    "    chunk_file = os.path.join(tempfile.gettempdir(), tile_id + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "    with open(chunk_file, 'wb') as f:\n",
    "        body = storage.get_cloudobject(chunk_cloudobject)\n",
    "        f.write(body)\n",
    "    \n",
    "    with rasterio.open(chunk_file, 'r') as chunk_src:\n",
    "        profile = chunk_src.profile\n",
    "        \n",
    "        # Interpolate variables from meteo station data, generate raster with result\n",
    "        if data_field == 'temp':\n",
    "            dest_chunk_file = os.path.join(tempfile.gettempdir(), tile_id + '_temp_' + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "            with rasterio.open(dest_chunk_file, 'w', **profile) as chunk_dest:\n",
    "                elevations = chunk_src.read(1)  # Get elevations content\n",
    "                interpolation = compute_basic_interpolation(elevations.shape, filtered, 'tdet', (offset_h, offset_w))\n",
    "                interpolation += r * (elevations - zdet)\n",
    "                chunk_dest.write(np.where(elevations == src.nodata, np.nan, interpolation), 1)\n",
    "        elif data_field == 'humi':\n",
    "            dest_chunk_file = os.path.join(tempfile.gettempdir(), tile_id + '_humi_' + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "            interpolation = compute_basic_interpolation((height, width), filtered, 'hr', (offset_h, offset_w))\n",
    "            chunk_dest.write(interpolation, 1)\n",
    "        elif data_field == 'wind':\n",
    "            dest_chunk_file = os.path.join(tempfile.gettempdir(), tile_id + '_wind_' + str(block_x) + '_' + str(block_y) + '.tif')\n",
    "            interpolation = compute_basic_interpolation((height, width), filtered, 'v', (offset_h, offset_w))\n",
    "            chunk_dest.write(interpolation, 1)\n",
    "        else:\n",
    "            raise Exception(f'Unknown data field \"{data_field}\"')\n",
    "\n",
    "    # Upload results to storage as Cloudobject\n",
    "    with open(dest_chunk_file, 'rb') as f:\n",
    "        co = storage.put_cloudobject(body=f, bucket=DATA_BUCKET)\n",
    "    \n",
    "    return [(tile_key, data_field, block_x, block_y, co)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lithops serverless computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T06:56:29.270356Z",
     "start_time": "2021-04-08T06:56:27.066344Z"
    }
   },
   "outputs": [],
   "source": [
    "# fs1 = fexec.map(radiation_interpolation, chunks)\n",
    "fs2 = fexec.map(map_interpolation, chunks, extra_args=('temp',))\n",
    "# fs3 = fexec.map(map_interpolation, iterdata, extra_args=(SPLITS, 'humi'))\n",
    "# fs4 = fexec.map(map_interpolation, iterdata, extra_args=(SPLITS, 'wind'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = fexec.get_result(fs=fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = fexec.get_result(fs=fs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_flat = [fs for sublist in [fs1, fs2, fs3, fs4] for fs in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = fexec.get_result(fs=fs_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPIs (Interpolation)\n",
    "\n",
    "[Skip KPI section](#(End-of-KPI-section---Interpolation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:02:05.040966Z",
     "start_time": "2021-04-08T07:02:04.321663Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.plot(dst=\"plots/interpolation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:02:45.854968Z",
     "start_time": "2021-04-08T07:02:45.848629Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/interpolation_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:02:54.046898Z",
     "start_time": "2021-04-08T07:02:54.036980Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/interpolation_timeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of .tif files being processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:29.258771Z",
     "start_time": "2021-04-08T07:03:29.255051Z"
    }
   },
   "outputs": [],
   "source": [
    "mdts_gtiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total size accounting that files were repeatedly processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:33.021230Z",
     "start_time": "2021-04-08T07:03:32.728522Z"
    }
   },
   "outputs": [],
   "source": [
    "data_size = sum(obj[\"Size\"] for obj in cloud_storage.list_objects(BUCKET) if obj[\"Key\"] in mdts_gtiff)\n",
    "data_size *= 4  # Each file was processed 4 times\n",
    "\n",
    "print(f\"Data size: {data_size / 1024**2} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:37.464531Z",
     "start_time": "2021-04-08T07:03:37.441706Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:51.895064Z",
     "start_time": "2021-04-08T07:03:51.879123Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(fexec.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:39.286038Z",
     "start_time": "2021-04-08T07:03:39.276118Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_interpolation = get_process_cost(fexec)\n",
    "print(f\"The experiment cost ${cost_interpolation:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:05:08.953755Z",
     "start_time": "2021-04-08T07:05:08.931643Z"
    }
   },
   "outputs": [],
   "source": [
    "tstamps = set()\n",
    "for future in fexec.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"tstamp\"):\n",
    "            tstamps.add(future.stats[key])\n",
    "            \n",
    "duration = max(tstamps) - min(tstamps)\n",
    "print(\"Duration: \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:05:11.960461Z",
     "start_time": "2021-04-08T07:05:11.953193Z"
    }
   },
   "outputs": [],
   "source": [
    "throughput_interpolation = data_size / duration  # Bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:05:13.342171Z",
     "start_time": "2021-04-08T07:05:13.338300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Throughput: {throughput_interpolation / 1024**2} MiB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we compare the execution speed of a sample process performed in last section, using different amounts of parallel workers, in order to test the scalability of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:39:50.231357Z",
     "start_time": "2021-04-13T14:39:50.225667Z"
    }
   },
   "outputs": [],
   "source": [
    "parallel_workers = [12, 24, 48, 72]\n",
    "experiment_duration = dict.fromkeys(parallel_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform experiment several times and save duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:41:52.721071Z",
     "start_time": "2021-04-13T14:40:15.651918Z"
    }
   },
   "outputs": [],
   "source": [
    "for option in parallel_workers:\n",
    "    fexec = lithops.FunctionExecutor(\n",
    "        backend=COMPUTE_BACKEND, \n",
    "        storage=STORAGE_BACKEND,\n",
    "        runtime=RUNTIME,\n",
    "        workers=option, # Tells lithops to work w/only this number of concurrent workers\n",
    "        log_level=\"DEBUG\"\n",
    "    )\n",
    "    fexec.map(\n",
    "        map_interpolation, iterdata, extra_args=(SPLITS,'temp'), runtime_memory=2048\n",
    "    )\n",
    "    fexec.get_result()\n",
    "    \n",
    "    tstamps = set()\n",
    "    for future in fexec.futures:\n",
    "        for key in future.stats.keys():\n",
    "            if key.endswith(\"tstamp\"):\n",
    "                tstamps.add(future.stats[key])\n",
    "    duration = max(tstamps) - min(tstamps)\n",
    "    experiment_duration[option] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:41:55.994144Z",
     "start_time": "2021-04-13T14:41:55.989057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of per-worker performance relative to first experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot represents two lines:\n",
    "- **Ideal speedup**: theoretical best speedup - scenario where a 2x increment in workers results in 1/2 execution time, a 4x increment in workers results in a 1/4 execution time, etc.\n",
    "- **Lithops speedup**: actual speedup that results from the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T15:09:52.888690Z",
     "start_time": "2021-04-13T15:09:52.723516Z"
    }
   },
   "outputs": [],
   "source": [
    "duration = list(experiment_duration.values())\n",
    "theoretical_best_speedup = [(1 - parallel_workers[0] / parallel_workers[i]) * 100 for i in range(0, len(parallel_workers))]\n",
    "actual_speedup = [(1 - duration[i] / duration[0]) * 100 for i in range(0, len(duration))]\n",
    "\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    theoretical_best_speedup\n",
    ")\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    actual_speedup\n",
    ")\n",
    "plt.xlabel(\"Number of workers\")\n",
    "plt.ylabel(\"% time reduced, relative to first experiment\")\n",
    "plt.legend([\"Ideal speedup\", \"Lithops speedup (this experiment)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:11:22.584754Z",
     "start_time": "2021-04-08T07:11:22.577217Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.futures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:10:54.929461Z",
     "start_time": "2021-04-08T07:10:54.917364Z"
    }
   },
   "source": [
    "###### (End of KPI section - Interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join split subsets into a tile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:21.921233Z",
     "start_time": "2021-04-08T07:21:21.911497Z"
    }
   },
   "outputs": [],
   "source": [
    "def gather_blocks(tile, splits, data_field, storage):\n",
    "\n",
    "    from rasterio.windows import Window\n",
    "    \n",
    "    # Get width and height from original tile\n",
    "    with rasterio.open(storage.get_object(bucket=BUCKET, key=f'MDT/{tile}.tif', stream=True)) as og:\n",
    "        height = og.profile['height']\n",
    "        width = og.profile['width']\n",
    "    \n",
    "    chunk_tiles = storage.list_keys(bucket=BUCKET, prefix=f'tmp/{data_field}/{tile}/chunk')\n",
    "        \n",
    "    # Open first object to obtain profile metadata\n",
    "    with rasterio.open(storage.get_object(bucket=BUCKET, key=chunk_tiles[0], stream=True)) as src:\n",
    "        profile = src.profile\n",
    "        profile.update(width=width)\n",
    "        profile.update(height=height)\n",
    "\n",
    "    # Iterate each object and print its block into the destination file\n",
    "    with rasterio.open(\"output\", \"w\", **profile) as dest: \n",
    "        for chunk in chunk_tiles:\n",
    "            j, i = os.path.splitext(os.path.basename(chunk))[0].rsplit('_')[1].split('-')\n",
    "            j, i = int(j), int(i)\n",
    "            with rasterio.open(storage.get_object(bucket=BUCKET, key=chunk, stream=True)) as src:\n",
    "                step_w = math.floor(width / splits)\n",
    "                step_h = math.floor(height / splits)\n",
    "                curr_window = Window(round(step_w * i), round(step_h * j), src.width, src.height)\n",
    "                content = src.read(1)\n",
    "                dest.write(content, 1, window=curr_window)\n",
    "            # storage.delete_object(bucket=BUCKET, key=chunk)\n",
    "    \n",
    "    output_key = os.path.join('tmp', data_field, tile, '_'.join([tile, data_field.upper()+'.tif']))\n",
    "    with open('output', 'rb') as out_file:\n",
    "        storage.put_object(bucket=BUCKET, key=output_key, body=out_file)  \n",
    "    \n",
    "    return output_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine previous split subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:43.580625Z",
     "start_time": "2021-04-08T07:21:43.180508Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fexec.map(gather_blocks, tiles, extra_args=(SPLITS, 'extrad'), runtime_memory=2048)\n",
    "\n",
    "fexec.map(gather_blocks, tiles, extra_args=(SPLITS, 'humi'), runtime_memory=2048)\n",
    "\n",
    "fexec.map(gather_blocks, tiles, extra_args=(SPLITS, 'rad'), runtime_memory=2048)\n",
    "\n",
    "fexec.map(gather_blocks, tiles, extra_args=(SPLITS, 'temp'), runtime_memory=2048)\n",
    "\n",
    "fexec.map(gather_blocks, tiles, extra_args=(SPLITS, 'wind'), runtime_memory=2048)\n",
    "\n",
    "out_combined = fexec.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPIs (Gather blocks)\n",
    "\n",
    "[Skip KPI section](#(End-of-KPI-section---Gather-blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:23:42.594365Z",
     "start_time": "2021-04-08T07:23:42.135958Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.plot(dst=\"plots/gather_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:23:44.168346Z",
     "start_time": "2021-04-08T07:23:44.162948Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/gather_blocks_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:23:45.856957Z",
     "start_time": "2021-04-08T07:23:45.840851Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/gather_blocks_timeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:17:25.463278Z",
     "start_time": "2021-04-08T08:17:25.458630Z"
    }
   },
   "outputs": [],
   "source": [
    "mdts_gtiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:17:30.709533Z",
     "start_time": "2021-04-08T08:17:29.875330Z"
    }
   },
   "outputs": [],
   "source": [
    "data_size = sum(obj[\"Size\"] for obj in cloud_storage.list_objects(BUCKET) if obj[\"Key\"] in mdts_gtiff)\n",
    "data_size *= 4  # Each file was processed 4 times\n",
    "\n",
    "print(f\"Data size: {data_size / 1024**2} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:18:54.513927Z",
     "start_time": "2021-04-08T08:18:54.486157Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:19:02.264749Z",
     "start_time": "2021-04-08T08:19:02.236457Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(fexec.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:21:09.051314Z",
     "start_time": "2021-04-08T08:21:09.031537Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_gather_blocks = get_process_cost(fexec)\n",
    "print(f\"The experiment cost ${cost_gather_blocks:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:19:57.208334Z",
     "start_time": "2021-04-08T08:19:57.189780Z"
    }
   },
   "outputs": [],
   "source": [
    "tstamps = set()\n",
    "for future in fexec.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"tstamp\"):\n",
    "            tstamps.add(future.stats[key])\n",
    "            \n",
    "duration = max(tstamps) - min(tstamps)\n",
    "print(\"Duration: \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:21:20.407233Z",
     "start_time": "2021-04-08T08:21:20.404210Z"
    }
   },
   "outputs": [],
   "source": [
    "throughput_gather_blocks = data_size / duration  # Bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:21:35.726076Z",
     "start_time": "2021-04-08T08:21:35.720504Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Throughput: {throughput_gather_blocks / 1024**2} MiB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:22:57.849484Z",
     "start_time": "2021-04-08T08:22:57.844112Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.futures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (End of KPI section - Gather blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of potential evaporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:51.493674Z",
     "start_time": "2021-04-13T16:57:51.485032Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_crop_evapotranspiration(temperatures,\n",
    "                                    humidities,\n",
    "                                    wind_speeds,\n",
    "                                    external_radiations,\n",
    "                                    global_radiations,\n",
    "                                    KCs):\n",
    "    gamma = 0.665*101.3/1000\n",
    "    eSat = 0.6108 * np.exp((17.27*temperatures)/(temperatures+237.3))\n",
    "    delta = 4098 * eSat / np.power((temperatures + 237.3),2)\n",
    "    eA = np.where(humidities < 0, 0, eSat * humidities / 100)     # Avoid sqrt of a negative number\n",
    "    T4 = 4.903 * np.power((273.3 + temperatures),4)/1000000000\n",
    "    rSrS0 = global_radiations/(external_radiations * 0.75)\n",
    "    rN = 0.8* global_radiations-T4*(0.34-0.14*np.sqrt(eA))*((1.35*rSrS0)-0.35)\n",
    "    den = delta + gamma *(1 + 0.34* wind_speeds)\n",
    "    tRad = 0.408 * delta * rN / den\n",
    "    tAdv = gamma * (900/(temperatures+273))*wind_speeds * (eSat - eA)/den\n",
    "    return ((tRad + tAdv) * 7 * KCs).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:52.760441Z",
     "start_time": "2021-04-13T16:57:52.753812Z"
    }
   },
   "outputs": [],
   "source": [
    "vineyard = ['VI', 'VO', 'VF', 'FV', 'CV' ]\n",
    "olive_grove = ['OV', 'VO', 'OF', 'FL', 'OC']\n",
    "fruit = ['FY', 'VF', 'OF', 'FF', 'CF']\n",
    "nuts = ['FS', 'FV', 'FL', 'FF', 'CS' ]\n",
    "citrus = ['CI', 'CV', 'OC', 'CF', 'CS' ]\n",
    "\n",
    "def get_kc(feature):\n",
    "    \n",
    "    # TODO: Get more precise values of Kc\n",
    "    sigpac_use = feature['properties']['uso_sigpac']\n",
    "    if sigpac_use in vineyard:\n",
    "        # Grapes for wine - 0.3, 0.7, 0.45\n",
    "        return 0.7  \n",
    "    if sigpac_use in olive_grove:\n",
    "        # Olive grove - ini: 0.65, med: 0.7, end: 0.7\n",
    "        return 0.7 \n",
    "    if sigpac_use in fruit:\n",
    "        # Apples, Cherries, Pears - 0.45, 0.95, 0.7\n",
    "        return 0.95\n",
    "    if sigpac_use in nuts:\n",
    "        # Almonds - 0.4, 0.9, 0.65\n",
    "        return 0.9\n",
    "    if sigpac_use in citrus:\n",
    "        # Citrus, without ground coverage - 0.7, 0.65, 0.7\n",
    "        return 0.65\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:54.250115Z",
     "start_time": "2021-04-13T16:57:54.243932Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_geometry_window(src, geom_bounds):\n",
    "    left, bottom, right, top = geom_bounds\n",
    "    src_left, src_bottom, src_right, src_top = src.bounds\n",
    "    window = src.window(max(left,src_left), max(bottom,src_bottom), min(right,src_right), min(top,src_top))\n",
    "    window_floored = window.round_offsets(op='floor', pixel_precision=3)\n",
    "    w = math.ceil(window.width + window.col_off - window_floored.col_off)\n",
    "    h = math.ceil(window.height + window.row_off - window_floored.row_off)\n",
    "    return Window(window_floored.col_off, window_floored.row_off, w, h)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:57.781920Z",
     "start_time": "2021-04-13T16:57:57.770029Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_evapotranspiration_by_shape(tem, hum, win, rad, extrad, dst):\n",
    "    \n",
    "    import fiona\n",
    "    from shapely.geometry import shape, box\n",
    "    from rasterio import features\n",
    "    \n",
    "    non_arable_land = ['AG', 'CA', 'ED', 'FO', 'IM', 'PA', 'PR', 'ZU', 'ZV']\n",
    "    \n",
    "    with fiona.open('zip://shape.zip') as shape_src:\n",
    "        for feature in shape_src.filter(bbox=tem.bounds):\n",
    "            KC = get_kc(feature) \n",
    "            if KC is not None:   \n",
    "                geom = shape(feature['geometry'])  \n",
    "                window = get_geometry_window(tem, geom.bounds)              \n",
    "                win_transform = rasterio.windows.transform(window, tem.transform)\n",
    "                # Convert shape to raster matrix\n",
    "                image = features.rasterize([geom],\n",
    "                                           out_shape=(window.height, window.width),\n",
    "                                           transform = win_transform,\n",
    "                                           fill = 0,\n",
    "                                           default_value = 1).astype('bool')\n",
    "                # Get values to compute evapotranspiration\n",
    "                temperatures = tem.read(1, window=window)\n",
    "                humidities = hum.read(1, window=window)\n",
    "                wind_speeds = win.read(1, window=window)\n",
    "                # Convert from W to MJ (0.0036)\n",
    "                global_radiations = rad.read(1, window=window) * 0.0036\n",
    "                external_radiations = extrad.read(1, window=window) * 0.0036\n",
    "                KCs = np.full(temperatures.shape, KC)\n",
    "                # TODO: compute external radiation\n",
    "                #external_radiations = np.full(temperatures.shape, 14)\n",
    "                # TODO: compute global radiation\n",
    "                # global_radiations = np.full(temperatures.shape, 10)\n",
    "                etc = compute_crop_evapotranspiration(\n",
    "                        temperatures,\n",
    "                        humidities,\n",
    "                        wind_speeds,\n",
    "                        external_radiations,\n",
    "                        global_radiations,\n",
    "                        KCs\n",
    "                )\n",
    "                etc[temperatures == tem.nodata] = dst.nodata\n",
    "                etc[np.logical_not(image)] = dst.nodata\n",
    "                dst.write(etc + dst.read(1, window=window), 1, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:59.216824Z",
     "start_time": "2021-04-13T16:57:59.207435Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_global_evapotranspiration(tem, hum, win, rad, extrad, dst):    \n",
    "    for ji, window in tem.block_windows(1):\n",
    "        bounds = rasterio.windows.bounds(window, tem.transform)\n",
    "        temperatures = tem.read(1, window=window)\n",
    "        humidities = hum.read(1, window=window)\n",
    "        wind_speeds = win.read(1, window=window)\n",
    "         # Convert from W to MJ (0.0036)\n",
    "        global_radiations = rad.read(1, window=window) * 0.0036\n",
    "        external_radiations = extrad.read(1, window=window) * 0.0036\n",
    "        # TODO: compute external radiation\n",
    "        #external_radiations = np.full(temperatures.shape, 14)\n",
    "        # TODO: compute global radiation\n",
    "        # global_radiations = np.full(temperatures.shape, 10)\n",
    "        # TODO: compute KCs\n",
    "        KCs = np.full(temperatures.shape, 1)\n",
    "        etc = compute_crop_evapotranspiration(\n",
    "                temperatures,\n",
    "                humidities,\n",
    "                wind_speeds,\n",
    "                external_radiations,\n",
    "                global_radiations,\n",
    "                KCs\n",
    "        )\n",
    "        dst.write(np.where(temperatures == tem.nodata, dst.nodata, etc), 1, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:58:01.128416Z",
     "start_time": "2021-04-13T16:58:01.101842Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_calculations(tile, storage):\n",
    "    \n",
    "    from functools import partial\n",
    "      \n",
    "    # Download shapefile\n",
    "    shapefile = storage.get_object(bucket=BUCKET, key='shapefile.zip', stream=True)\n",
    "    with open('shape.zip', 'wb') as shapf:\n",
    "        for chunk in iter(partial(shapefile.read, 200 * 1024 * 1024), ''):\n",
    "            if not chunk:\n",
    "                break\n",
    "            shapf.write(chunk)\n",
    "    \n",
    "    temp = storage.get_object(bucket=BUCKET, key=f'tmp/temp/{tile}/{tile}_TEMP.tif', stream=True)\n",
    "    humi = storage.get_object(bucket=BUCKET, key=f'tmp/humi/{tile}/{tile}_HUMI.tif', stream=True)\n",
    "    rad = storage.get_object(bucket=BUCKET, key=f'tmp/rad/{tile}/{tile}_RAD.tif', stream=True)\n",
    "    extrad = storage.get_object(bucket=BUCKET, key=f'tmp/extrad/{tile}/{tile}_EXTRAD.tif', stream=True)\n",
    "    wind = storage.get_object(bucket=BUCKET, key=f'tmp/wind/{tile}/{tile}_WIND.tif', stream=True)\n",
    "    \n",
    "    with rasterio.open(temp) as temp_raster:\n",
    "        with rasterio.open(humi) as humi_raster:\n",
    "            with rasterio.open(rad) as rad_raster:\n",
    "                with rasterio.open(extrad) as extrad_raster:\n",
    "                    with rasterio.open(wind) as wind_raster:\n",
    "                        profile = temp_raster.profile\n",
    "                        profile.update(nodata=0)\n",
    "        \n",
    "                        with rasterio.open('output', 'w+', **profile) as dst:\n",
    "#                             compute_global_evapotranspiration(temp_raster, humi_raster, wind_raster,\n",
    "#                                                               rad_raster, extrad_raster, dst)\n",
    "                            compute_evapotranspiration_by_shape(temp_raster, humi_raster, wind_raster,\n",
    "                                                                rad_raster, extrad_raster, dst)\n",
    "    \n",
    "    out_key = f'etc/{tile}_ETC.tif'\n",
    "    with open('output', 'rb') as output_f:\n",
    "        storage.put_object(bucket=BUCKET, key=out_key, body=output_f)\n",
    "    return out_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:58:55.822484Z",
     "start_time": "2021-04-13T16:58:54.943303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fexec.map(combine_calculations, tiles, runtime_memory=2048)\n",
    "\n",
    "res = fexec.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPIs (Potential evaporation)\n",
    "\n",
    "[Skip KPI section](#(End-of-KPI-section---Potential-evaporation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:06:46.567960Z",
     "start_time": "2021-04-13T20:06:45.870566Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.plot(dst=\"plots/potential_evaporation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:04.735119Z",
     "start_time": "2021-04-13T20:07:04.729026Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/potential_evaporation_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:08.400698Z",
     "start_time": "2021-04-13T20:07:08.395328Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/potential_evaporation_timeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:08:12.027411Z",
     "start_time": "2021-04-13T20:08:12.018635Z"
    }
   },
   "outputs": [],
   "source": [
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:54:33.848670Z",
     "start_time": "2021-04-13T20:54:33.645465Z"
    }
   },
   "outputs": [],
   "source": [
    "data_size = 0\n",
    "\n",
    "for obj in cloud_storage.list_objects(BUCKET):\n",
    "    for tile in tiles:\n",
    "        if obj[\"Key\"] == f'tmp/temp/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/humi/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/rad/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/extrad/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/wind/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == 'shapefile.zip':\n",
    "            data_size += obj[\"Size\"]\n",
    "\n",
    "print(f\"Data size: {data_size / 1024**2} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:54:45.609853Z",
     "start_time": "2021-04-13T20:54:45.592066Z"
    }
   },
   "outputs": [],
   "source": [
    "fexec.job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:16.935146Z",
     "start_time": "2021-04-13T20:07:16.918524Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(fexec.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:26.235760Z",
     "start_time": "2021-04-13T20:07:26.226071Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_potential_evaporation = get_process_cost(fexec)\n",
    "print(f\"The experiment cost ${cost_potential_evaporation:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:55:04.595689Z",
     "start_time": "2021-04-13T20:55:04.581005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tstamps = set()\n",
    "for future in fexec.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"tstamp\"):\n",
    "            tstamps.add(future.stats[key])\n",
    "            \n",
    "duration = max(tstamps) - min(tstamps)\n",
    "print(\"Duration: \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:55:07.194755Z",
     "start_time": "2021-04-13T20:55:07.192029Z"
    }
   },
   "outputs": [],
   "source": [
    "throughput_potential_evaporation = data_size / duration  # Bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:55:09.220719Z",
     "start_time": "2021-04-13T20:55:09.212358Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Throughput: {throughput_potential_evaporation / 1024**2} MiB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we compare the execution speed of a sample process performed in last section, using different amounts of parallel workers, in order to test the scalability of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:10:15.560833Z",
     "start_time": "2021-04-13T21:10:15.556295Z"
    }
   },
   "outputs": [],
   "source": [
    "parallel_workers = [2, 4, 8]\n",
    "experiment_duration = dict.fromkeys(parallel_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform experiment several times and save duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:26:47.958535Z",
     "start_time": "2021-04-13T21:10:17.409017Z"
    }
   },
   "outputs": [],
   "source": [
    "for option in parallel_workers:\n",
    "    fexec = lithops.FunctionExecutor(\n",
    "        backend=COMPUTE_BACKEND, \n",
    "        storage=STORAGE_BACKEND, \n",
    "        runtime=RUNTIME,\n",
    "        workers=option, # Tells lithops to work w/only this number of concurrent workers\n",
    "        log_level=\"NOTSET\"\n",
    "    )\n",
    "    fexec.map(combine_calculations, tiles, runtime_memory=2048)\n",
    "    fexec.get_result()\n",
    "    \n",
    "    tstamps = set()\n",
    "    for future in fexec.futures:\n",
    "        for key in future.stats.keys():\n",
    "            if key.endswith(\"tstamp\"):\n",
    "                tstamps.add(future.stats[key])\n",
    "    duration = max(tstamps) - min(tstamps)\n",
    "    experiment_duration[option] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:29:20.900597Z",
     "start_time": "2021-04-13T21:29:20.893752Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of per-worker performance relative to first experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot represents two lines:\n",
    "- **Ideal speedup**: theoretical best speedup - scenario where a 2x increment in workers results in 1/2 execution time, a 4x increment in workers results in a 1/4 execution time, etc.\n",
    "- **Lithops speedup**: actual speedup that results from the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:29:33.018923Z",
     "start_time": "2021-04-13T21:29:32.780739Z"
    }
   },
   "outputs": [],
   "source": [
    "duration = list(experiment_duration.values())\n",
    "theoretical_best_speedup = [(1 - parallel_workers[0] / parallel_workers[i]) * 100 for i in range(0, len(parallel_workers))]\n",
    "actual_speedup = [(1 - duration[i] / duration[0]) * 100 for i in range(0, len(duration))]\n",
    "\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    theoretical_best_speedup\n",
    ")\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    actual_speedup\n",
    ")\n",
    "plt.xlabel(\"Number of workers\")\n",
    "plt.ylabel(\"% time reduced, relative to first experiment\")\n",
    "plt.legend([\"Ideal speedup\", \"Lithops speedup (this experiment)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fexec.futures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (End of KPI section - Potential evaporation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:30:27.539704Z",
     "start_time": "2021-04-13T21:30:26.276005Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "tile = random.choice(tiles)\n",
    "obj = io.BytesIO(cloud_storage.get_object(bucket=BUCKET, key=f'etc/{tile}_ETC.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:32:01.429761Z",
     "start_time": "2021-04-13T21:32:00.245568Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "with rasterio.open(obj) as src:\n",
    "    arr = src.read(1, out_shape=(src.height, src.width))\n",
    "    ax.set_title(tile)\n",
    "    img = ax.imshow(arr, cmap='Greens')\n",
    "    fig.colorbar(img, shrink=0.5)\n",
    "\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "obj.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T10:06:07.571233Z",
     "start_time": "2021-03-29T10:05:14.526Z"
    }
   },
   "outputs": [],
   "source": [
    "# keys = cloud_storage.list_keys(bucket=BUCKET, prefix='')\n",
    "# keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T10:06:07.574068Z",
     "start_time": "2021-03-29T10:05:14.528Z"
    }
   },
   "outputs": [],
   "source": [
    "# for key in keys:\n",
    "#     cloud_storage.delete_object(bucket=BUCKET, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
